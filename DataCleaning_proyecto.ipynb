{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAREAS DE LIMPIEZA DE DATOS\n",
    "1. **Cambiar las abreviaturas -> ejem: \"u\"->you \"ur->\"your\"**\n",
    "2. Buscar  caracteres Nulls y Remplazar con valor \"0\" (El cero debe ser valor String). Igual queda en Duda y se debria preguntar al tutor y o demas compa;eros\n",
    "3. Buscar las palabras con mayor frecuencia y hacer un grafica o Datavisualitation.\n",
    "4. **Quitar stopwords ejem: the, and, that, a, any, an, be, with...**\n",
    "5. eliminar emojis, URL, hashtags, contracciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('prueba.csv', delimiter = ';', names = ['Conversation_Id', 'Authors', 'Class', 'Conversation' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df['Conversation'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazar | por un espacio ya que solo consideraremos las palabras presentes más no las secuencias de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: x.replace('|', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir números a palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "df_conv = df_conv.apply(lambda x: ' '.join([num2words(word) if word.isnumeric() and int(word)<1000000000 else word for message in x.split('|') for word in message.split(' ')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazar doble espacio con uno solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: ' '.join([word.lower() for word in x.split(' ') if word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar abreviaciones del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_dict={\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "    \"whats\":\"what is\",\n",
    "    \"whatre\":\"what are\",\n",
    "    \"whos\":\"who is\",\n",
    "    \"whore\":\"who are\",\n",
    "    \"wheres\":\"where is\",\n",
    "    \"wherere\":\"where are\",\n",
    "    \"whens\":\"when is\",\n",
    "    \"whenre\":\"when are\",\n",
    "    \"hows\":\"how is\",\n",
    "    \"howre\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "    \"im\":\"i am\",\n",
    "    #\"were\":\"we are\",\n",
    "    \"youre\":\"you are\",\n",
    "    \"theyre\":\"they are\",\n",
    "    \"hes\":\"he is\",\n",
    "    \"shes\":\"she is\",\n",
    "    \"thats\":\"that is\",\n",
    "    \"theres\":\"there is\",\n",
    "    \"therere\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "    \"ive\":\"i have\",\n",
    "    \"weve\":\"we have\",\n",
    "    \"youve\":\"you have\",\n",
    "    \"theyve\":\"they have\",\n",
    "    \"whove\":\"who have\",\n",
    "    \"wouldve\":\"would have\",\n",
    "    \"notve\":\"not have\",\n",
    "\n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "    \"ill\":\"i will\",\n",
    "    #\"well\":\"we will\",\n",
    "    \"youll\":\"you will\",\n",
    "    \"hell\":\"he will\",\n",
    "    \"shell\":\"she will\",\n",
    "    \"itll\":\"it will\",\n",
    "    \"theyll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\n",
    "    \"isnt\":\"is not\",\n",
    "    \"wasnt\":\"was not\",\n",
    "    \"arent\":\"are not\",\n",
    "    \"werent\":\"were not\",\n",
    "    \"cant\":\"can not\",\n",
    "    \"couldnt\":\"could not\",\n",
    "    \"dont\":\"do not\",\n",
    "    \"didnt\":\"did not\",\n",
    "    \"shouldnt\":\"should not\",\n",
    "    \"wouldnt\":\"would not\",\n",
    "    \"doesnt\":\"does not\",\n",
    "    \"havent\":\"have not\",\n",
    "    \"hasnt\":\"has not\",\n",
    "    \"hadnt\":\"had not\",\n",
    "    \"wont\":\"will not\",\n",
    "\n",
    "    \"2mrw\": \"tomorrow\",\n",
    "    \"aka\": \"also known as\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"a/s/l\": \"age sex location\",\n",
    "    \"ayt\": \"are you there\",\n",
    "    \"b4\": \"before\",\n",
    "    \"bbs\": \"be back soon\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"cmb\": \"call me back\",\n",
    "    \"cu l8r\": \"see you later\",\n",
    "    \"cul8r\": \"see you later\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"cos\": \"because\",\n",
    "    \"cwyl\": \"chat with you later\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"f2f\": \"face to face\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"ig\": \"instagram\",\n",
    "    \"fyeo\": \"for your eyes only\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"diy\": \"do it yourself\",\n",
    "    \"stfu\": \"shut the fuck up\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"ily\": \"ily\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"lmfao\": \"laughing my freaking ass off\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"ikr\": \"i know right\",\n",
    "    \"ofc\": \"of course\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"k\": \"okay\",\n",
    "    \"r\": \"are\",\n",
    "    \"n\": \"and\",\n",
    "    \"b\": \"be\",\n",
    "    \"wat\": \"what\",\n",
    "    \"ya\": \"you\",\n",
    "    \"dunno\": \"do not know\",\n",
    "    \"yea\": \"yeah\",\n",
    "    \"ok\": \"okay\",\n",
    "    \"kewl\": \"cool\",\n",
    "    \"nite\": \"night\",\n",
    "    \"yep\": \"yes\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: ' '.join([abbr_dict[word] if word in abbr_dict.keys() else word for word in x.split(' ') ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df_conv = df_conv.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar puntuación de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar valores alfanumericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: ' '.join([word for word in x.split(' ') if word.isalpha()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "df_conv = df_conv.apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in x.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Conversation'] = df_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.dropna(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predators = df[df['Class'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_predators.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "common_words = Counter(\" \".join(df_predators[\"Conversation\"]).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(common_words).to_csv(\"Most_common_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_pred = [item[0] for item in common_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df['Conversation'].apply(lambda x: ' '.join([word for word in x.split(' ') if word in words_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.','This document is the second document.','And this is the third one.', 'Is this the first document?']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df1.to_list())\n",
    "#print(len(vectorizer.get_feature_names()))\n",
    "df_final = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "df_final['Class'] = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"Features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ec4eee48a1b90125fedd080afd04f965bb6f28dbf2886da6c5cfcc343d0a26d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
